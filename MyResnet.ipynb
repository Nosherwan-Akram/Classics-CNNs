{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1394
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 784981,
     "status": "ok",
     "timestamp": 1532895035458,
     "user": {
      "displayName": "Nosherwan Akram",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110124425388121978011"
     },
     "user_tz": -300
    },
    "id": "I3rZRW9vDZEX",
    "outputId": "e0dbf97f-53b2-416d-8b54-f4c6655955cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "50000/50000 [==============================] - 132s 3ms/step - loss: 2.0274 - acc: 0.3207\n",
      "Epoch 2/40\n",
      "40576/50000 [=======================>......] - ETA: 23s - loss: 1.8597 - acc: 0.358650000/50000 [==============================] - 123s 2ms/step - loss: 1.8179 - acc: 0.3704\n",
      "Epoch 3/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 1.7132 - acc: 0.4104\n",
      "Epoch 4/40\n",
      " 9216/50000 [====>.........................] - ETA: 1:40 - loss: 1.5639 - acc: 0.450550000/50000 [==============================] - 123s 2ms/step - loss: 1.5745 - acc: 0.4585\n",
      "Epoch 5/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 1.5319 - acc: 0.4603\n",
      "Epoch 6/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 1.3066 - acc: 0.5373\n",
      "Epoch 7/40\n",
      "46080/50000 [==========================>...] - ETA: 9s - loss: 1.1357 - acc: 0.597550000/50000 [==============================] - 123s 2ms/step - loss: 1.1349 - acc: 0.5986\n",
      "Epoch 8/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 1.0249 - acc: 0.6396\n",
      "Epoch 9/40\n",
      "10880/50000 [=====>........................] - ETA: 1:36 - loss: 0.9442 - acc: 0.674750000/50000 [==============================] - 123s 2ms/step - loss: 0.9262 - acc: 0.6773\n",
      "Epoch 10/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.8048 - acc: 0.7202\n",
      "Epoch 11/40\n",
      "  384/50000 [..............................] - ETA: 2:01 - loss: 0.6336 - acc: 0.791750000/50000 [==============================] - 123s 2ms/step - loss: 0.7094 - acc: 0.7555\n",
      "Epoch 12/40\n",
      "45952/50000 [==========================>...] - ETA: 9s - loss: 0.5989 - acc: 0.7921 50000/50000 [==============================] - 123s 2ms/step - loss: 0.6106 - acc: 0.7884\n",
      "Epoch 13/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.4738 - acc: 0.8333\n",
      "Epoch 14/40\n",
      "10624/50000 [=====>........................] - ETA: 1:36 - loss: 0.3258 - acc: 0.891850000/50000 [==============================] - 123s 2ms/step - loss: 0.3645 - acc: 0.8726\n",
      "Epoch 15/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.3067 - acc: 0.8977\n",
      "Epoch 16/40\n",
      "  256/50000 [..............................] - ETA: 2:03 - loss: 0.1596 - acc: 0.949250000/50000 [==============================] - 123s 2ms/step - loss: 0.2138 - acc: 0.9266\n",
      "Epoch 17/40\n",
      "45824/50000 [==========================>...] - ETA: 10s - loss: 0.1848 - acc: 0.935850000/50000 [==============================] - 123s 2ms/step - loss: 0.1869 - acc: 0.9352\n",
      "Epoch 18/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.1364 - acc: 0.9535\n",
      "Epoch 19/40\n",
      "10496/50000 [=====>........................] - ETA: 1:36 - loss: 0.0834 - acc: 0.970950000/50000 [==============================] - 123s 2ms/step - loss: 0.1131 - acc: 0.9600\n",
      "Epoch 20/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.1019 - acc: 0.9643\n",
      "Epoch 21/40\n",
      "  256/50000 [..............................] - ETA: 2:02 - loss: 0.0802 - acc: 0.964850000/50000 [==============================] - 123s 2ms/step - loss: 0.0968 - acc: 0.9657\n",
      "Epoch 22/40\n",
      "45824/50000 [==========================>...] - ETA: 10s - loss: 0.0803 - acc: 0.972150000/50000 [==============================] - 123s 2ms/step - loss: 0.0806 - acc: 0.9720\n",
      "Epoch 23/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.0669 - acc: 0.9767\n",
      "Epoch 24/40\n",
      "10496/50000 [=====>........................] - ETA: 1:36 - loss: 0.0653 - acc: 0.977850000/50000 [==============================] - 123s 2ms/step - loss: 0.0715 - acc: 0.9752\n",
      "Epoch 25/40\n",
      "50000/50000 [==============================] - 122s 2ms/step - loss: 0.0696 - acc: 0.9757\n",
      "Epoch 26/40\n",
      "  256/50000 [..............................] - ETA: 2:01 - loss: 0.0456 - acc: 0.976650000/50000 [==============================] - 123s 2ms/step - loss: 0.0633 - acc: 0.9784\n",
      "Epoch 27/40\n",
      "45824/50000 [==========================>...] - ETA: 10s - loss: 0.0582 - acc: 0.979950000/50000 [==============================] - 123s 2ms/step - loss: 0.0609 - acc: 0.9789\n",
      "Epoch 28/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.0519 - acc: 0.9821\n",
      "Epoch 29/40\n",
      "10496/50000 [=====>........................] - ETA: 1:36 - loss: 0.0557 - acc: 0.980350000/50000 [==============================] - 123s 2ms/step - loss: 0.0555 - acc: 0.9807\n",
      "Epoch 30/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.0544 - acc: 0.9807\n",
      "Epoch 31/40\n",
      "  256/50000 [..............................] - ETA: 2:00 - loss: 0.0359 - acc: 0.984450000/50000 [==============================] - 123s 2ms/step - loss: 0.0537 - acc: 0.9815\n",
      "Epoch 32/40\n",
      "45824/50000 [==========================>...] - ETA: 10s - loss: 0.0376 - acc: 0.987650000/50000 [==============================] - 123s 2ms/step - loss: 0.0398 - acc: 0.9872\n",
      "Epoch 33/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.0493 - acc: 0.9825\n",
      "Epoch 34/40\n",
      "10496/50000 [=====>........................] - ETA: 1:37 - loss: 0.0590 - acc: 0.979450000/50000 [==============================] - 124s 2ms/step - loss: 0.0486 - acc: 0.9831\n",
      "Epoch 35/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.0369 - acc: 0.9876\n",
      "Epoch 36/40\n",
      "  256/50000 [..............................] - ETA: 2:03 - loss: 0.0436 - acc: 0.976650000/50000 [==============================] - 123s 2ms/step - loss: 0.0446 - acc: 0.9848\n",
      "Epoch 37/40\n",
      "45824/50000 [==========================>...] - ETA: 10s - loss: 0.0380 - acc: 0.986750000/50000 [==============================] - 123s 2ms/step - loss: 0.0394 - acc: 0.9863\n",
      "Epoch 38/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.0330 - acc: 0.9885\n",
      "Epoch 39/40\n",
      "10496/50000 [=====>........................] - ETA: 1:37 - loss: 0.0380 - acc: 0.986450000/50000 [==============================] - 123s 2ms/step - loss: 0.0413 - acc: 0.9857\n",
      "Epoch 40/40\n",
      "50000/50000 [==============================] - 123s 2ms/step - loss: 0.0326 - acc: 0.9893\n",
      "  160/10000 [..............................] - ETA: 1:1910000/10000 [==============================] - 13s 1ms/step\n",
      "0.6722\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "x_train_mean = np.mean(train_images, axis=0)\n",
    "train_images -= x_train_mean\n",
    "test_images -= x_train_mean\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "input_shape = train_images.shape[1:]\n",
    "inputs = Input(shape=input_shape)\n",
    "x = inputs\n",
    "x = Conv2D(64,kernel_size=7,strides=1,padding='same')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "\n",
    "#stage1\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(64,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(64,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(64,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(64,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(64,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(64,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#stage2\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(128,kernel_size=3,strides=2,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(128,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x_shortcut = Conv2D(128,kernel_size=1,strides=2,padding='same')(x_shortcut)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(128,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(128,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(128,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(128,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(128,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(128,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#stage3\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(256,kernel_size=3,strides=2,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x_shortcut = Conv2D(256,kernel_size=1,strides=2,padding='same')(x_shortcut)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(256,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#stage5\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(512,kernel_size=3,strides=2,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(512,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x_shortcut = Conv2D(512,kernel_size=1,strides=2,padding='same')(x_shortcut)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(512,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(512,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_shortcut = x\n",
    "x = Conv2D(512,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(512,kernel_size=3,strides=1,padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.add([x, x_shortcut])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = AveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(10,activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)#, name='my resnet')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=40, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gFPOGSfLDZEf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "MyResnet.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
